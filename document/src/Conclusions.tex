\chapter{Conclusions \& Future Work}
\label{sec:conclusions}
Uses of virtualization have consistently grown as new technologies have improved performance and the overhead of virtualization has decreased.  
Containers are a recent innovation in virtualization that has been a catalyst for change in the industry as NFV and new computing paradigms are developed.
The open-source operating system Linux has seen significant improvement in both traditional virtualization and the creation of new mechanisms for process isolation.  
This innovation has led to new use cases and orchestration mechanisms to manage and control virtual systems.

As expected, the Docker containers studied generally had higher bandwidth and lower latency than virtual machines.
In almost all of the streaming TCP tests both systems realized a reduction to variance in their bandwidth and latency.
Virtual machines and containers tended to perform similarly, however, when the physical network interfaces were dedicated to the virtual system.
The reduction in variance often comes at the cost of a small decrease in performance, but, as in the case of the TCP STREAM test over a bridged interface, throughput may improve in some cases.

Although bridged interfaces saw a greater improvement in most cases than the physical interfaces, it was also shown to have more variability and lower throughput than the physical network connections.  
This paradigm is a concern for large numbers of guests because any networking topology that requires host kernel involvement cannot scale to a large number of guests without a performance impact.
This is because the work required by the host to monitor and handle bridge traffic and layer 3 decisions for multiple guests can be significant and induces additional latency for each guest added to the bridge.    
Bridged interfaces are extremely flexible and versatile, but the computation required by the host kernel indicates that a large number of client systems would see significant contention for the bridge, limiting its potential for scale out.

Specific innovations such as the preempt-rt patches for Linux have enabled levels of determinism that were previously only available in embedded systems with dedicated hardware.  
Application of the \texttt{preempt-rt} patches and targeted tuning of the host and guest systems was shown to enable some increases in TCP bandwidth as well as reduction of variance in TCP request/response latency, especially in network interfaces such as bridges which require host kernel involvement.   
While no work was performed to quantify their contributions, isolating processes from the OS general scheduler seems to yield a significant improvement to their performance.
This trend of dedicating hardware to individual processes should only increase as the core counts of server systems increase.
A system running a processor with many CPU cores, such as the 60-core Xeon Phi, could be the server core of the future in this context.

\section{Future Work}
\label{sec:futurework}

\subsection{UDP Flows} % (fold)
\label{sec:futureudpflows}
Chapters~\ref{sec:experimental_design} and \ref{sec:results} described unexpected performance and required workarounds when UDP performance was measured with \texttt{netperf}. 
It seemed that the system under test was configured with settings that prevented the UDP flows from transmitting at the expected rate, but the root cause of this behavior is still unknown.  
Since \texttt{netperf} is one of the most common network performance tools, the solution to the unexpected behavior could be a valuable discovery.
% subsection futureudpflows (end)

\subsection{Virtual Functions} % (fold)
\label{sec:futurevirtualfunctions}
Virtual functions are a mechanism by which physical devices may be multiplexed and share their resources among many virtual devices.  
While virtual functions may subdivide the resources of the physical device, a virtual function belonging to a peripheral such as a network interface card operates at line rate and should attain latency similar to the physical function.  
Although scaling to tens of containers may not be possible with physical device assignment, a small set of physical devices may be subdivided into virtual functions, enabling near-line-rate performance for all of the containers.  
The performance of virtual functions should be investigated in the context of process isolation and system tuning to verify that containers can scale out to the levels advertised in the press.  
% subsection futurevirtualfunctions (end)

\subsection{DPDK} % (fold)
\label{sec:futuredpdk}
The isolation of processes from the rest of the system has been shown to improve performance.
User space libraries such as DPDK can allow systems to minimize interference by the kernel by removing the interfaces from the kernel domain and using poll mode user space drivers to minimize response latency \autocite{_dpdk_1}.  
This paradigm represents the limit of process and hardware isolation because applications written with the DPDK library monopolize the network interfaces and CPU cores, consuming extra hardware resources to maximize performance.
Optimizing parameters of the network stack in Linux or implementing benchmarks in DPDK are additional optimizations for improving network performance and should be considered in future work.
% subsection futuredpdk (end)

% future work: 
%Pin interrupts from network interface to cores for user-space processes.
%i(future work) OVS, if it uses user-space drivers like DPDK, can benefit even more.
%co-located processed, shared memory communication.
%interference studies with this tuning should show robust against shared resources.
%CPU util as a functino of contention - multicore testing with IRQ pinning.
%Pinning bridge to cores?
%(future work is to check out application (macro benchmark) performance using these)

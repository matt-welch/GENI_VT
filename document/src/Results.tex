\chapter{Results}
\label{cha:results}
\label{sec:results}
Network performance of Docker containers and KVM/QEMU virtual machines was analyzed using standard benchmarking tools including netperf and ping.  
The results of the tests are presented as box plots with the median in the center, 25th and 75th percentiles at the bottom and top bounds of the box and min and max at the ends of the error bars.
This plot helps to illustrate the one-sided variance that may often be observed in network performance analysis where measurements can only approach the theoretical maxima for each test.  

Variation in the measured latency is partly due to the non-determinism of typical operating systems, but there is also a component of the latency due only to the network delays.
The 10 Gbps optical cables connecting the two systems are approximately 2 meters long so the propagation delay along these connections should be about 6.7 ns.
In-network queuing delay can be ignored since the systems are directly connected and queuing is only occurring at their network interfaces.
The transmission or packetization delay for transmitting or receiving a large 1500 byte packet would be 1.214 $\mu$s.
This represents as much as 3\% of the total latency observed herein, but this offset should be constant across all tests and have little effect on the variance observed. 

\section{Network Bandwidth} % (fold)
\label{sec:networkbandwidth}
Network bandwidth was measured for all test configurations with both TCP and UDP streaming flows.
It should be noted, however, that the maximum theoretical speed for Ethernet connections, both optical and copper, is just below 95\% of the physical data rate.
This is due only to the overhead of default MTU Ethernet frames (1518 bytes + 8 byte preamble and 12 byte (96 ns) interframe gap (IFG)), Ethernet header (14 bytes), 4 byte Frame Check Sequence (FCS), 20 byte IP header, 20 byte TCP header, 1460 MSS for TCP.
Overall, this amounts to 78 bytes of overhead at various layers to transmit only 1460 bytes of data representing 94.9\% efficiency in the best case.  
Thus, any performance above 9.4 Gbps (9400 Mbps) is considered line rate.  
The overhead fraction can, of course, be reduced with jumbo Ethernet frames, but that level of network tuning is outside the scope of this study.

\subsection{TCP Bandwidth} % (fold)
\label{sub:tcpbandwidth}
Bandwidth of the various network interfaces and kernels was measured using the TCP\_STREAM bandwidth test in netperf.
The results of the bridged measurements are shown in Figure~\ref{fig:tcp_stream_bridge}. 
It is evident in this figure that the streaming TCP bandwidth of both environments improves with tuning as well as decreases in variance.  
It is even fairly impressive that the realtime container reached line rate with only a few points below the median.  
The performance of both the container and virtual machine are consistent with a realtime kernel in that the performance is essentially deterministic with minimal variance.
This is the result that is desired when tuning a system for deterministic performance.

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{TCP_STREAM_BRG.pdf_tex}
    \caption{TCP Streaming Bandwidth: Bridged Network}
    \label{fig:tcp_stream_bridge}
\end{figure}

The results of the TCP STREAM test on the physical connection are shown in Figure~\ref{fig:tcp_stream_phys}.
This figure shows exceptional performance from the virtual environments with all of the variants reaching line rate.
While line rate performance was somewhat expected in the physical interfaces, even the standard kernel variants show exceptional performance here.
The realtime tuned kernel decreases variance in this case, but, since the baseline performance is so high already, little to no gain is observed in the median performance, with all configurations less than 0.01\% separated from each other.  
The differences between the bridged and physical network performance seen in Figures \ref{fig:tcp_stream_bridge} and \ref{fig:tcp_stream_phys}, respectively, partly illustrate the risks of using host bridging for networking due to the additional latency contributed by a higher density of guest systems on that host.  

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{TCP_STREAM_PHY.pdf_tex}
    \caption{TCP Streaming Bandwidth: Physical Network}
    \label{fig:tcp_stream_phys}
\end{figure}
% subsection tcpbandwidth (end)

\subsection{UDP Bandwidth} % (fold)
\label{sub:udpbandwidth}
Although a significant fraction of datacenter traffic has been shown to use TCP, one of the greatest sources of traffic in the Internet is streaming video and audio which are typically UDP flows.
UDP usually uses less computation per byte of goodput since flow control and loss correction are not part of the protocol.  
This would seem to indicate that UDP performance should be equivalent or higher than TCP performance.  
The UDP bandwidth results observed herein, however, are somewhat confusing and contrary to that expectation.

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{UDP_STREAM_BRG.pdf_tex}
    \caption{UDP Streaming Bandwidth: Bridged Network}
    \label{fig:udp_stream_bridge}
\end{figure}
Figure~\ref{fig:udp_stream_bridge} shows the UDP bandwidth for the bridged network connections.  
In one of the more confusing results of this study, the UDP STREAM test could not complete over a bridged connection to a container.
Regardless of attempts to vary the routing tables or connections between systems, the UDP STREAM test over a bridge to a container consistently failed to initiate.
The virtual machine, however, with its paravirtualized \texttt{virtio} driver over a bridge did not seem to have any issues completing the test.  
Its performance, however, is significantly lower than line-rate expectations, but the variance in bandwidth decreased dramatically from the standard to realtime kernel as expected.  

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{UDP_STREAM_PHY.pdf_tex}
    \caption{UDP Streaming Bandwidth: Physical Network}
    \label{fig:udp_stream_phys}
\end{figure}

The second UDP bandwidth test performed was over the physical network interface is shown in Figure~\ref{fig:udp_stream_phys} .  
This figure shows that both virtual systems perform similarly when using a physical interface, achieving line rate and nearly zero variance in the realtime systems.
As expected, the standard kernel systems had about 15\% lower median performance, peaking up to line rate in the virtual machine.
This test further illustrates the strange UDP behavior in that the peak bandwidth of the two realtime systems and the standard virtual machine all exceeded 9.5 Gbps. 
This is confusing in light of the maximum Ethernet bandwidth calculation described at the beginning of Section~\ref{sec:networkbandwidth}.
It seems that some burst transmission or variation in the packet size is happening for UDP streaming, which may bias the result higher than expected performance.
When comparing the results of the UDP STREAM test over a physical connection to that of the bridged connection in Figure~\ref{fig:udp_stream_bridge}, it seems that the bridged virtual machine interface simply suffered reduced performance as a result of the additional host CPU involvement required to process bridge traffic.

% subsection udpbandwidth (end)


% section networkbandwidth (end)

\section{Network Latency} % (fold)
\label{sec:networklatency}
Bandwidth measurements get a lot of attention due to the desire to push more packets through the high-bandwidth connections in datacenters and network backbones.
Latency and latency jitter however, are often the critical metric when evaluating a network workload because low bandwidth will not prevent streaming services from running effectively whereas high latency can cause dropped calls or prevent connectivity in the first place.
In this section, the results of netperf and ping latency measurements are discussed.  

\subsection{TCP Latency} % (fold)
\label{sub:tcplatency}
The netperf test TCP RR (Request/Response) and UDP RR both measure the time required to initiate then tear down a network connection, i.e. the latency between the first SYN packet is sent and the last FIN packet is ACKed.  
This test only sends a few packets so congestion should not be a concern in this test environment, but CPU involvement in the packet processing may comprise a significant portion of the latency.  
Figure~\ref{fig:tcp_rr_bridge} shows the TCP Request-Response latency of the bridged topologies.
This figure perfectly illustrates the intended result for tuning a system toward realtime performance.
Both the virtual machine and container show a significant reduction in variance when using realtime over standard kernel.
The realtime Docker result does not show much decrease in its median latency, but variance decreases dramatically as expected.
The virtual machine, however, showed reductions in latency as well as its variance.

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{TCP_RR_BRG.pdf_tex}
    \caption{TCP Request/Response Latency: Bridged Network}
    \label{fig:tcp_rr_bridge}
\end{figure}

Measurements of TCP RR latency over a physical interface are shown in Figure~\ref{fig:tcp_rr_phys}.  
Similar to the bridged TCP latency, the physical interface shows improvement moving from the standard to realtime kernel, but all four modes seem to perform fairly similarly here regardless.
This appears to be a trend in the physical interfaces where performance is more consistent across interfaces and virtual systems.
What is confusing, however, is that the median latency of the virtual machine seemed to show an increase for the realtime kernel, but a decrease in its variance.  
This may be normal with a realtime kernel, however, where improved determinism may come at the cost of maximum performance.  
It also seems strange that the Docker measurements here do not seem to vary at all.  
They initially appeared to be the same data, but on closer inspection, differ by less than 5\% at the minima and less than 1\% at the maxima.

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{TCP_RR_PHY.pdf_tex}
    \caption{TCP Request/Response Latency: Physical Network}
    \label{fig:tcp_rr_phys}
\end{figure}

% subsection tcplatency (end)
\subsection{UDP Latency} % (fold)
\label{sub:udplatency}
Although the TCP measurements presented so far seem to be fairly consistent with expectations, the UDP workloads have not been as straightforward.
The latency of both of the UDP Request-Response tests are exemplary of the strange behavior seen herein.
When measuring network latency, it is expected that the majority of the delay will come from transit across the network and queuing delays at intermediate nodes along the way.  
The UDP latency shown in Figure~\ref{fig:udp_rr_bridge}, however, shows that network effects can be trivially small in comparison to other delays in the system.
These latency measurements are roughly 10 times the magnitude of the latency observed in the TCP case.
This effect does not seem possible without some system interference causing the additional delays.  
As before, these measurements were repeated multiple times with the same result each time, indicating that there is some persistent configuration issue slowing down the UDP test.
Further investigation into the source code of netperf or system network parameters may uncover the issue, but it is difficult to draw any further conclusions from these data at this time.

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{UDP_RR_BRG.pdf_tex}
    \caption{UDP Request/Response Latency: Bridged Network}
    \label{fig:udp_rr_bridge}
\end{figure}
Results of the UDP Request-Response measurements over a physical interface are shown in Figure~\ref{fig:udp_rr_phys}.  
This figure shows the same curiously-high latency as observed over the bridged connection, but at least the physical interface has lower variance than the bridge as expected.
The Docker results are additionally confusing due to the increase in variance observed in the realtime tuned variant over the standard Docker system.
They seem to have been switched, but the data was consistent upon inspection which creates more questions than it answers.
The virtual machine results are also troubling because both measurements are so incredibly precise at 8004 $\pm$ 0.5 $\mu$s.  
This level of precision seems too deterministic, especially for the standard kernel so it is assumed that netperf or some kernel configuration is interfering with the maximum performance of the measurements.

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{UDP_RR_PHY.pdf_tex}
    \caption{UDP Request/Response Latency: Physical Network}
    \label{fig:udp_rr_phys}
\end{figure}
% subsection udplatency (end)

\subsection{ICMP Latency} % (fold)
\label{sub:icmplatency}
In order to get an idea of how the whole system responds to a realtime kernel, ICMP latency was also measured with the standard network tool ping.  
The ping tool presents a slightly different case than netperf due to the location of the responding agent.  
Since netperf is a userspace process, its CPU affinity may be set to any CPU in the system, specifically those that have been isolated from the kernel.
ICMP responses, however, come directly from the kernel's network stack so cannot be easily assigned to a CPU, which limits how isolated its performance can be from the rest of the system.
The results of the ping testing of the bridged connections are shown in Figure~\ref{fig:icmplatencybridge}.
It's interesting that the container performance improved only slightly with the realtime kernel, whereas the virtual machine variance improved significantly at the cost of higher median latency.  
The difference in effect is likely due to the CPU affinity of the virtual machine.  
The standard kernel virtual machine sees very high peak latency due to the additional abstraction layers that must be traversed for each packet and the nondeterministic floating CPU cores that might respond.
When the VM is pinned to specific, isolated CPU cores as in the realtime example, the host kernel is not involved in the ICMP response so it does not contribute to the variance.
The containers, however, do not have this same benefit because, while the container processes may be pinned to its cgroup cpuset, the host kernel is still doing the responding so should behave almost identically for both configurations.

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{ICMP_LAT_BRG.pdf_tex}
    \caption{ICMP latency: Bridged network}
    \label{fig:icmplatencybridge}
\end{figure}
Aside from the expected decrease in latency from the bridged interfaces to the physical, the results of the physical interface ping testing are very similar to those of the bridged interface, Figure~\ref{fig:icmplatencyphys}.  
This is due to the same constraints mentioned in the discussion of the bridged ICMP results where the host kernel is only slightly affected by the tuning, but the guest kernel gets private cores to run on, allowing a modest decrease in both variance and magnitude.  
Both of these ICMP latency test serve to illustrate that ping is only marginally helpful at assessing the effect of kernel tuning on guest systems.  

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{ICMP_LAT_PHY.pdf_tex}
    \caption{ICMP latency: Physical network}
    \label{fig:icmplatencyphys}
\end{figure}
% subsection icmplatency (end)

% section networklatency (end)

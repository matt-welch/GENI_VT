\chapter{Results}
\label{sec:results}
Network performance of Docker containers and KVM/QEMU virtual machines was analyzed using standard benchmarking tools including \texttt{netperf} and \texttt{ping}.  
The results of the tests are presented as box plots with the median in the center, 25th and 75th percentiles at the bottom and top bounds of the box and minimum and maximum at the ends of the error bars.
This plot helps to illustrate the one-sided variance that may often be observed in network performance analysis where measurements can only approach the theoretical maxima for each test.  

\section{Network Bandwidth} % (fold)
\label{sec:networkbandwidth}
Network bandwidth was measured for all test configurations with both TCP and UDP streaming flows.
It should be noted, however, that the maximum theoretical speed for Ethernet connections, both optical and copper, is just below 95\% of the physical data rate.
This is due to the overhead of default MTU Ethernet frames (1518 bytes + 8 byte preamble and 12 byte (96 ns) interframe gap (IFG)), Ethernet header (14 bytes), 4 byte Frame Check Sequence (FCS), 20 byte IP header, 20 byte TCP header, 1460 MSS for TCP.
Overall, this amounts to 78 bytes of overhead at various layers to transmit 1460 bytes of data representing 94.9\% efficiency in the best case.  
Thus, any performance above 9.4 Gbps (9400 Mbps) is considered line rate.  
The overhead fraction can be reduced with jumbo Ethernet frames, but that level of network tuning is outside the scope of this study.

\subsection{TCP Bandwidth} % (fold)
\label{sub:tcpbandwidth}
Bandwidth of the various network interfaces and kernels was measured using the TCP STREAM bandwidth test in \texttt{netperf}.
The results of the bridged measurements are shown in Figure~\ref{fig:tcp_stream_bridge}. 
It is evident in this figure that the streaming TCP bandwidth of both environments improves with tuning as well as decreases in variance.  
It is even fairly impressive that the realtime container reached line rate with only a few points below the median.  
The performance of both the container and virtual machine are consistent with a realtime kernel in that the performance is essentially deterministic with minimal variance.
This is the result that is desired when tuning a system for deterministic performance.

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{TCP_STREAM_BRG.pdf_tex}
    \caption{TCP Streaming Bandwidth: Bridged Network}
    \label{fig:tcp_stream_bridge}
\end{figure}

The results of the TCP STREAM test on the physical connection are shown in Figure~\ref{fig:tcp_stream_phys}.
This figure shows exceptional performance from the virtual environments with all of the variants reaching line rate.
While line rate performance was somewhat expected in the physical interfaces, even the standard kernel variants show exceptional performance here.
The realtime tuned kernel decreases variance in this case, but, since the baseline performance is so high already, little to no gain is observed in the median performance, with all configurations less than 0.01\% separated from each other.  

The differences between the bridged and physical network performance seen in Figures \ref{fig:tcp_stream_bridge} and \ref{fig:tcp_stream_phys}, respectively, partly illustrate the risks of using host bridging for networking due to the additional latency contributed by a higher density of guest systems on that host.  

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{TCP_STREAM_PHY.pdf_tex}
    \caption{TCP Streaming Bandwidth: Physical Network}
    \label{fig:tcp_stream_phys}
\end{figure}
% subsection tcpbandwidth (end)

\subsection{UDP Bandwidth} % (fold)
\label{sub:udpbandwidth}
Although a significant fraction of data center traffic has been shown to use TCP \autocite{haTCPCloud2013}, one of the greatest sources of traffic in the Internet is streaming video and audio which are typically UDP flows.
UDP uses less computation per byte of goodput since flow control and loss correction are not part of the protocol.  
This seems to indicate that UDP performance should be equivalent or higher than TCP performance.  
The UDP bandwidth results observed here, however, are contrary to that expectation.

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{UDP_STREAM_BRG.pdf_tex}
    \caption{UDP Streaming Bandwidth: Bridged Network}
    \label{fig:udp_stream_bridge}
\end{figure}
Figure~\ref{fig:udp_stream_bridge} shows the UDP bandwidth for the bridged network connections.  
As described in Section~\ref{sec:experimental_procedure}, however, the UDP STREAM test has some important details that should be understood before interpreting its performance.
Unexpectedly, the most critical of these is that \texttt{netperf} disables IP routing for the UDP STREAM tests by default.
On inspection of the netperf source code, it was found that this setting was ``grudgingly added'' to prevent accidentally flooding default routes and corporate networks with UDP STREAM packets \autocite{netperfsource}.
IP routing is enabled in all UDP STREAM tests here to facilitate routing across separate subnets for each interface.
The maximum UDP bandwidth of the four test systems is much lower than that observed in the TCP STREAM tests, contrary to expectations.
Low bandwidth is a direct result of limiting the message size as described in Section~\ref{sec:experimental_procedure}.
Although peak bandwidth on this test can be much higher with a larger message size, the results of that test variation are difficult to reproduce, which necessitates limiting the message size to obtain consistent results.  
Containers showed minimal variance in this test for both kernels, but improved performance for the realtime kernel.  
This indicates that the computational cost of determining bridge actions in the kernel is not a significant source of latency at this message size and resulting data rate.
Although the VM here showed an increased bandwidth in the realtime kernel, the range of observed measurements also increases from 722 to 964 Mbps. 
This first set of UDP results is indicative of the complications encountered in this thesis during UDP testing.

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{UDP_STREAM_PHY.pdf_tex}
    \caption{UDP Streaming Bandwidth: Physical Network}
    \label{fig:udp_stream_phys}
\end{figure}

The second UDP bandwidth test performed was over the physical network interface is shown in Figure~\ref{fig:udp_stream_phys} .  
These results show how significantly bandwidth is limited here by the restriction of the UDP message size.
Since all four systems show similar maxima and variance, it appears that the system load provided with this test variation is minimal.
Although slightly higher performing than the bridged interfaces, the maximum bandwidth observed here, 4.43 Gbps in the realtime Docker test, is only 1.5\% higher than the maximum bandwidth observed in the bridged interface, 4.37 Gbps for the realtime VM. 
None of the UDP bandwidth results here approach the theoretical limit of 9.5 Gbps nor do they vary significantly between test conditions.
Given the lack of variation among any of the test systems observed here, it is difficult to draw any conclusions here other than that the workload did not provide a significant challenge to the systems.
The potential for improving these conditions is discussed further in Section~\ref{sec:futureudpflows}.

% subsection udpbandwidth (end)

% section networkbandwidth (end)

\section{Network Latency} % (fold)
\label{sec:networklatency}
Bandwidth measurements get a lot of attention due to the desire to push more packets through the high-bandwidth connections in data centers and network backbones.
Latency and jitter however, are often the critical metric when evaluating a network workload because low bandwidth does not prevent streaming services from running effectively whereas high latency can cause dropped calls or prevent connectivity in the first place.
In this section, the results of \texttt{netperf} and \texttt{ping} latency measurements are discussed.  

Variation in the measured latency is partly due to non-determinism normally present in operating systems, but there is also a component of the latency due only to the network delays.
The 10 Gbps optical cables connecting the two systems are approximately 2 meters long so the propagation delay along these connections should be about 6.7 ns.
In-network queuing delay can be ignored since the systems are directly connected and queuing is only occurring at their network interfaces.
The transmission or packetization delay for transmitting or receiving a large byte packet is 1.214 $\mu$s.
This represents as much as 3\% of the total latency observed here, but this offset should be constant across all tests and have little effect on the observed variance. 

The \texttt{netperf} TCP Request/Response (RR) subtest and UDP RR subtest both measure the number pf transactions that can be completed for a given request and response size during a specified time period \autocite{netperfTraining}.
Each transaction comprises the exchange of a single request and a single response so the average round trip latency can be determined from the transaction rate.  
The time required to initiate then tear down a network connection is not included with this test.  
Only a few packets are sent with this test so congestion is not a concern in this environment, but CPU involvement in the packet processing may comprise a significant portion of the latency.  

\subsection{TCP Latency} % (fold)
\label{sub:tcplatency}
Figure~\ref{fig:tcp_rr_bridge} shows the TCP RR latency of the bridged interfaces.
This figure illustrates the intended result for tuning a system toward realtime performance.
Both the virtual machine and container show a significant reduction in variance when using a realtime kernel instead of a standard kernel.
The realtime Docker result does not show much decrease in its median latency, but variance decreases dramatically as expected.
The virtual machine, however, showed reductions in latency as well as its variance.

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{TCP_RR_BRG.pdf_tex}
    \caption{TCP Request/Response Latency: Bridged Network}
    \label{fig:tcp_rr_bridge}
\end{figure}

Measurements of TCP RR latency over a physical interface are shown in Figure~\ref{fig:tcp_rr_phys}.  
Similar to the bridged TCP RR latency, the physical interface shows improvement moving from the standard to a realtime kernel, but all four modes seem to perform similarly.
This appears to be a trend in the physical interfaces where performance is more consistent across interfaces and virtual systems.
Variance within each environment here is not especially low, but the physical interface is much more consistent between systems than the bridged interface, due to fewer abstraction layers involved in processing packets on this interface. 
What is surprising, however, is that the median latency of the virtual machine shows an increase for the realtime kernel, but a decrease in its variance.  
This may be normal with a realtime kernel, however, where improved determinism may come at the cost of maximum performance.  
It also seems unusual that the Docker measurements here do not seem to vary.  
On closer inspection, they differ by less than 5\% at the minima and less than 1\% at the maxima.

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{TCP_RR_PHY.pdf_tex}
    \caption{TCP Request/Response Latency: Physical Network}
    \label{fig:tcp_rr_phys}
\end{figure}

% subsection tcplatency (end)

\subsection{UDP Latency} % (fold)
\label{sub:udplatency}

Although the TCP measurements presented so far are consistent with expectations, the UDP workloads have not been as straightforward.
The latency of both of the UDP Request-Response tests are exemplary of the unexpected behavior observed.
When measuring network latency, it is expected that the majority of the delay comes from transit across the network and queuing delays at intermediate nodes along the way.  
The UDP latency shown in Figure~\ref{fig:udp_rr_bridge}, however, shows that the network effects discussed previously can be insignificant in comparison to other delays in the system.
These UDP RR latency measurements are roughly 10 times the magnitude of the latency observed in the TCP RR case.
These measurements were repeated multiple times with similar results each time, indicating that there is a persistent configuration limiting the rate at which the UDP test can run.
UDP RR measurements were also performed to localhost as a check, resulting in latencies just above 200 $\mu$s (data not shown).  
These measurements seem to indicate that there exists a pervasive limit to UDP transaction rates in this system as it is configured.  
Investigation into the source code of \texttt{netperf} did not uncover any special treatment of the UDP RR tests leading to this behavior, which lends support to the hypothesis that system configuration may be responsible for the unexpected performance.

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{UDP_RR_BRG.pdf_tex}
    \caption{UDP Request/Response Latency: Bridged Network}
    \label{fig:udp_rr_bridge}
\end{figure}

Results of the UDP Request-Response measurements over a physical interface are shown in Figure~\ref{fig:udp_rr_phys}.  
This figure shows the same high latency as observed over the bridged connection, but at least the physical interface has lower variance than the bridge as expected.
The Docker results are additionally surprising due to the increase in variance observed in the realtime tuned variant over the standard Docker system.
The virtual machine results are also troubling because both measurements are very precise at 8004 $\pm$ 0.5 $\mu$s.  
This level of precision seems too deterministic, especially for the standard kernel so it is assumed that system configuration is interfering with the maximum transaction rate of the UDP RR measurements.

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{UDP_RR_PHY.pdf_tex}
    \caption{UDP Request/Response Latency: Physical Network}
    \label{fig:udp_rr_phys}
\end{figure}

% subsection udplatency (end)

\subsection{ICMP Latency} % (fold)
\label{sub:icmplatency}

In order to get an idea of how the whole system responds to a realtime kernel, ICMP latency was also measured with the standard network tool \texttt{ping}.  
The \texttt{ping} tool presents a slightly different case than \texttt{netperf} due to the location of the responding agent.  
Since \texttt{netperf} is a userspace process, its CPU affinity may be set to any CPU in the system, specifically those that have been isolated from the kernel.
ICMP responses, however, come directly from the kernel's network stack so cannot be easily assigned to a CPU, which limits how isolated its performance can be from the rest of the system.
The results of the \texttt{ping} testing of the bridged connections are shown in Figure~\ref{fig:icmplatencybridge}.
It is interesting that the container performance improved only slightly with the realtime kernel, whereas the virtual machine variance improved significantly at the cost of higher median latency.  
The difference between the systems can be explained with an understanding of the CPU affinity of the virtual machine.  
When using the standard kernel, the virtual machine sees very high peak latency due to the additional abstraction layers that must be traversed for each packet and the nondeterministic floating CPU cores that might respond.
When the VM is pinned to specific, isolated CPU cores as in the realtime example, the host kernel is not involved in the ICMP response so it does not contribute to the variance.
The containers, however, do not have this same benefit.
Although processes running inside a container are pinned to the container's cgroup cpuset, the host kernel is responsible for responding to ICMP requests.
Putting the responding agent in the host kernel removes much of the benefit of isolating cores so both kernels should behave similarly in this test.

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{ICMP_LAT_BRG.pdf_tex}
    \caption{ICMP latency: Bridged network}
    \label{fig:icmplatencybridge}
\end{figure}
Aside from the expected decrease in latency from the bridged interfaces to the physical, the results of the physical interface \texttt{ping} testing are very similar to those of the bridged interface, Figure~\ref{fig:icmplatencyphys}.  
This is a result of the constraints mentioned in the discussion of the bridged ICMP results.
The host kernel latency is only slightly affected by kernel tuning, but the guest kernel gets private cores to run its processes, allowing a modest decrease in both variance and magnitude.  
Both of these ICMP latency tests serve to illustrate that \texttt{ping} is only marginally helpful at assessing the effect of kernel tuning on guest system performance.  

\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{ICMP_LAT_PHY.pdf_tex}
    \caption{ICMP latency: Physical network}
    \label{fig:icmplatencyphys}
\end{figure}
% subsection icmplatency (end)

% section networklatency (end)

In the preceding sections, the variance of TCP connections is shown to improve with a realtime kernel and tuning to enable process isolation.
Additionally, physical interfaces are shown to outperform bridged interfaces in almost all cases, demonstrating that bridged interfaces can create significant bottlenecks to performance.
UDP measurements shown here are not as illuminating, however.  
In each UDP test performed here, there exists some system configuration or program setting that complicates interpretation of the results.
Modification of the message size for the UDP STREAM tests was required in order to obtain consistent results.  
That had the effect of diminishing the value of those results in understanding the effects of system tuning.
Finally, the results of the ICMP testing serve to demonstrate that tuned kernels and system configurations can improve performance in most cases, but the environment and context where a process is running are equally important to performance as system configuration.


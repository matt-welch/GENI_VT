\chapter{Related Work}
\label{sec:related_work}
Virtualization has been an important part of computing since early mainframes needed to be shared among multiple processes.
The use cases for virtualization in network-resident workloads are numerous, but performance of virtual systems is often a concern.
Traditionally, virtual machines have been the common case for systems virtualization, with containers taking a secondary role as a process isolation mechanism.
Recent growth in containers, however, specifically the very popular Docker \autocite{dockerdotcom}, has fueled a resurgence in their uses in the enterprise.
Network Functions Virtualization (NFV) and cloud computing have also grown recently, driving performance improvements of virtual systems, including both virtual machines and containers.
In NFV, the functionality that used to be embodied in physical devices such as routers, firewalls, and load balancers can now be realized within a virtual system which increases overall network flexiblity and scalability in many cases.
There have been many comparisons between virtual systems, but they mostly lack any kind of tuning to improve network performance of these systems.
It is possible to improve the performance of virtual systems with fairly simple configuration changes to the kernel and modification of boot parameters.
This section discusses some of the enabling developments in virtual system architecture as well as some of the comparisons among them.  

\section{Use-Cases for Virtualization} % (fold)
\label{sec:usecasesvt}
The use cases for virtualization are numerous and growing. 
Recently, the ``cloud'' and cloud services have been the subject of significant development and interest.
Growth has been strong in cloud services for good reason \autocite{_younge_1}.
They describe many of the benefits offered by cloud services such as scalability, controlling quality of service (QoS), customization of user infrastructure, cost effectiveness, and simplified access interfaces, almost all of which are enabled by virtualization.  
Cloud infrastructure commonly requires large-scale data centers with high speed networking to support large numbers of systems.  
High Performance Computing (HPC) shares many of the infrastructure requirements and capabilities of these large computing centers so may also benefit from innovations originating in the cloud and virtual systems \autocite{xavier2013performance, _younge_1}.

TODO: cite OPNFV and ETSI NFV use-cases paper here.
\begin{enumerate}
  \item Virtualization as growth driver for data centers and enterprise networks
  \item NFV has recently become popular 
  \item Containers gain popularity as a distribution platform in parallel with explosion of NFV
\end{enumerate}

The popular networking paradigm of NFV is driving growth and innovation throughout the sector.
A number of trade organizations have sprouted up recently to promote its adoption and accelerate development.
Both the European Telecommunications Standards Institute (ETSI) and Open Platform for Network Functions Virtualization (OPNFV) \autocite{opnfv1} are driving NFV adoption \autocite{cohnopnfv}.
In their survey of network virtualization \autocite{_chowdhury_1}, Chowdhury and Boutaba describe some of the older technologies and challenges that define this problem space.


Recently, the Linux Foundation announced that they are fully supporting the efforts to promote and advance the development of realtime Linux \autocite{_linux_foundation_1}.
The Linux Foundation is the body that guides and sponsors the direction of Linux so their interest comes at an important time for realtime Linux due to recent funding difficuties for realtime Linux \autocite{_lwn_1} so their interest in realtime Linux provide a critical boost to its development and adoption.
Virtual systems also benefit from developments to realtime Linux due to improvements in system determinism and responsiveness.

To support high definition video of 340,000 packets per second into 4 Gbps streams, the British Broadcasting Corporation (BBC) is offloading packet processing into a userspace application to avoid the Linux Kernel TCP/IP stack \autocite{bbcbypass}, with CloudFlare using a similar userspace processing approach to offload packet inspection during packet flood attacks.
This allows servers to drop a large number of packets belonging to the flood and inject valid packets back into the kernel to be processed normally.  
The article did not mention which library BBC was using, but describes CloudFlare's solution as a modification of NetMap's functionality \autocite{netmapbertin}.
In order to meet their processing target window of only 3 $\mu$s, any latency due to network processing must be minimized.

% section usecasesvt (end)

\section{Virtual Machine Architecture}
\label{sec:vmarchitecture}
In the following sections, virtual machine architecture is discussed to demonstrate their resource needs and potential areas for improvement.

\subsection{Virtual Machines}
\label{sec:virtualmachines}
The architectures available in virtual environments are as widely varied as the types of hardware they seek to emulate.
Many of the concepts currently in use regarding virtual machines originated in a seminal 1974 paper by Popek and Goldberg \autocite{_popek_1}.
Although virtual machines had already been implemented on ``third-generation computer systems'' such as the IBM 360/67 by then, Popek and Goldberg sought to establish formal requirements for and prove whether other systems of that era were capable of supporting a virtual machine monitor (VMM)\autocite{_popek_1}.
At the time, their analysis focused on the possibility of a VMM, but the term hypervisor has largely come to replace VMM as the name of a software system that allocates, monitors, and controls virtual machine resources as an intermediary between the hardware and the virtual machine's operating system (OS).
What follows in this section is an introduction to some important concepts in hypervisor and virtual machine architecture and the work that led to them.

\subsection{Hypervisors}
\label{sec:hypervisors}
Hypervisors are available in a few flavors, depending on the where the hypervisor is running in relation to the hardware and the guest operating system.
The term ``guest'' here is used to refer to a virtual machine that is running on emulated or hosted hardware and the term ``host'' is used to refer to the hardware, operating system, and hypervisor.  
In a Type 1 hypervisor, the hypervisor is running directly on the CPU or ``bare metal''.
That is, the hypervisor code is not being hosted, translated, or controlled by another system or piece of software, but running as a native process on the CPU.
Type 2 hypervisors, however, require a host operating system to provide some system services including memory and file system management.

The fundamental difference between these two types of hypervisors is that a Type 1 hypervisor does not require an operating system to run, whereas a Type 2 does.
Although Type 1 hypervisors do not require an operating system to \emph{run}, they do require an operating system for \emph{control} of the hypervisor and guests \autocite{_liguori_1}.
Type 1 systems appear to have an efficiency advantage over Type 2 since they tend to use microkernels instead of the macrokernels that are usually required to host Type 2 hypervisors.
According to \autocite{_liguori_1}, however, the difference between them has little to do with performance, robustness, or other qualitative factors, but, rather, relates back to  observations about their differences made in \autocite{_popek_1} and the analyses of these differences by \autocite{_robin_1}.
Robin's analysis related to the potential for the Pentium processor to support a secure VMM \autocite{_robin_1}, so it was important to draw conclusions about the capabilities and suitability of various hypervisors.
The vendors of hypervisor solutions also have an interest in perpetuating the distinction between the two types of hypervisors, but the need to differentiate between these products has become less important over time.  

Although it's arguable whether there is a performance difference between Type 1 and Type 2 hypervisors, the market for virtualization is full of both.
VMware, one of the more popular enterprise virtualization vendors, has products covering both architectures including ESXi, their Type 1 enterprise hypervisor product that is responsible for running large-scale virtualized systems all around the world~\autocite{vmwareProducts}.
They also provide Type 2 hypervisors such as VMware Workstation Player that runs on systems from desktops to small-scale servers that do not need the higher levels of orchestration and performance offered by ESXi.

In addition to numerous other available hypervisors, the Kernel Virtual Machine (KVM) is the de facto hypervisor in Linux that has been included as a module in the Linux kernel since February 2007 when it was included with Linux kernel version 2.6.20 \autocite{_kvm_1}.
The KVM module allows the host operating system to provide the low-level hardware access to the guest that has been enabled by hardware virtualization extensions such as Intel's VT-x \autocite{_grinberg_1}.
Along with KVM in Linux, there is a user-space component known as the Quick EMUlator or QEMU\autocite{_qemu_1}.
QEMU was designed to be an architecture-agnostic emulator and virtualization helper, capable of running software written for one architecture on other architectures.
QEMU can achieve relatively good performance using binary translation, but, when paired with KVM or the Xen hypervisor, it can achieve near-native performance by executing guest instructions directly on the host processor.

Combined, the KVM/QEMU hypervisor runs natively on Linux with kernel modules and userspace tools, but additional layers such as libvirt \autocite{_libvirt_1} can be utilized to simplify VM creation, monitoring, and orchestration.


\subsection{CPU Virtualization}
\label{sec:vtcpu}
In addition to the variations in hypervisors that may or may not have an impact on performance, the type of virtualization used to provide devices to the guest operating system can have a significant impact on performance.
In the following sections, some important types of hardware emulation are reviewed comprising \texttt{virtio}, paravirtualization, and hardware assisted virtualization.

One of the early challenges in the virtualization of the x86 architecture was handling privileged instructions.
The x86 processor was originally designed with 4 ``rings'', numbered from 0 to 3, representing decreasing privilege levels as the rings increase.
Operating systems utilizing the x86 instruction set execute user code in ring 3 and privileged instructions (kernel code) in ring 0.
Virtual machines running on the x86 architecture execute their instructions as a user-space process in ring 3 so the host OS can maintain control of the system.
The mechanism by which the processor executes privileged instructions on behalf of the guest has been the topic of considerable effort in the advancement of virtualization which has spawned multiple techniques for handling these instructions.

One of the earliest methods, known as binary translation, involved trapping privileged guest instructions in the hypervisor and translating them into ``sequences of instructions that have the intended effect on the virtual hardware'' \autocite{vmwareVT}.
The binary translation technique, developed by VMware, was one of the most efficient methods in early hypervisors, but is now considered to have high overhead due to the additional time required to perform the code translation.
This same translation process, however, allows a hypervisor using this technique to run guest operating systems for virtually any processor on any other instruction set, provided that an efficient translation can be achieved.
This flexibility makes binary translation one of the most versatile methods of virtualization available which makes it well-suited for virtualization of old instruction sets or hardware.

Paravirtualization is another technique for handling guest privileged instructions.
It involves cooperation between the guest and host operating systems to improve efficiency.
The guest OS must be modified to replace privileged instructions ``with hypercalls that communicate directly with the virtualization layer hypervisor'' \autocite{vmwareVT}.
VMware has incorporated this method in their vmxnet series of network drivers to accelerate network workloads.
A more widely known example of paravirtualization is one of the first open-source hypervisors, known as the Xen hypervisor \autocite{_barham_1}.
At the risk of oversimplification, the Xen project is, essentially, a modified Linux host that communicates directly with the guest kernel.
The guest kernel and modules must also be modified in a paravirtualized system to facilitate this communication which places an additional burden on hardware vendors to provide not only open source drivers but also paravirtualized open source drivers for their hardware.
Although the Xen hypervisor was originally developed with the intent of being hardware agnostic, the modifications required to the guest operating system mean that only vendors wishing to participate in the open-source community provide paravirtualized drivers.
The community itself is free to develop these drivers, but this adds a barrier to adoption for most new hardware products.
Despite the additional effort required, Xen maintained their lead as a very popular open-source hypervisor for many years.
Paravirtualization has its fans, however, and much work has been done comparing Xen to containers and other hypervisors such as KVM \autocite{_felter_1, _younge_1, wangAmazon2010, _che_1, _scheepers_1, wangAllocation2007, rathore2013kvm}.

The third popular virtualization method is becoming the \emph{de facto} standard as virtualization matures.
Hardware assisted virtualization, which is heavily promoted by CPU vendors such as Intel and AMD, started out as an effort to accelerate instruction translation, but early generations of hardware had difficulty keeping up with the binary translation preferred by VMware \autocite{vmwareVT}.
The silicon vendors, however have been improving their virtualized performance and adding hooks to enable virtualization of their hardware.
Since operating systems for x86 were already common when hardware assisted virtualization was introduced, the architecture needed subtle modifications to help enable virtualization without breaking existing software.  
This was accomplished with the introduction of yet another protection ring that runs below the kernel's ring 0.  
The hypervisor runs in ring -1, below the operating system kernel, and trap privileged instructions when they are executed by the guest.
For each virtual machine, a new structure is created and maintained in the host kernel memory.  
Logically similar to a process control block, this structure, is commonly known as a Virtual Machine Control Structure (VMCS) or, alternately as a VM Control Block, (VMCB-AMD). 
The VMCS maintains the state of the virtual machine in the host kernel and is updated when the guest needs to perform a ``vm-exit'' to allow the host to execute privileged instructions.
This vm-exit is the process of a virtual machine preserving its CPU context then returning control of the CPU to the host OS and has been a significant source of latency for guest privileged instructions.
Advancements in virtualization-aware driver models like SR-IOV, however, have improved both latency and the frequency of these exits \autocite{_nasa_1}.

A deep discussion of the hardware systems and mechanisms of processor and platform virtualization outside the scope of this thesis.
Suffice it to say, however, that the CPU vendors have significant interest in producing higher core-count CPUs and supporting virtualization.
The discussion of these processors is limited even further to x86 architecture, but it should be noted that ARM and other vendors are actively working to enable virtualization with similar methods.
Additionally, important components of virtualization such as SR-IOV are managed by the Peripheral Component Interconnect Special Interest Group (PCI-SIG) and are not the exclusive domain of x86 architecture \autocite{_pcisig_1}.
To that end, both Intel and AMD have independently developed processor extensions to enable virtualization \autocite{_grinberg_1}.

\subsection{Memory Virtualization}
\label{sec:vt_memory}
Virtualization of the CPU was the first challenge in the development of secure virtualization, but, in Von Neumann processor architecture, the memory unit is equally important.
Memory has become an increasingly significant source of latency in processors as the CPU core has improved its performance faster than the improvement of memory, so improvements in this area are doubly beneficial for virtual machines.
Virtualization uses memory structures similar to those developed for virtual memory in early computing systems.
Virtual memory was created to allow multiprogramming and process address spaces that are larger than the available physical memory.
Modern CPUs implement virtual memory with the aid of a memory management unit (MMU) and translation lookaside buffer (TLB) to manage page tables and accelerate page lookups.
The host system must carefully control access to memory which holds the system state along with program data.
Fairly recent developments in x86 processors have allowed hypervisors to maintain guest memory mappings with shadow page tables, known as Extended Page Tables (EPTs) in Intel processors and Nested Page Tables (NPTs) for AMD.
A hypervisor may use TLB hardware to map guest memory pages onto the physical memory similar to a native process, reducing the overhead of guest OS memory access.
Along with virtualization of guest memory, another significant improvement to virtual machine performance can be had with the utilization of hugepage memory \autocite{_romer_1}.
Hugepages, referred to as superpages in \autocite{_romer_1}, can be described as a memory page that is a power-of-two multiple of a standard 4096 byte (4k) memory page.
Romer et al. demonstrated a significant improvement in performance when using hugepages for memory-hungry applications.
When system memory sizes were very small, virtual memory and swapping smaller pages was more efficient than larger pages.
As memory sizes have grown to hundreds of gigabytes per server and applications can consume multiple gigabytes each, 4k memory pages no longer seem like an obvious fit.
Hugepages seek to reduce the frequency of TLB lookups and page table walks by using larger pages of memory.
With respect to performance, they showed that TLB overhead could be reduced by as much as 99\% using \emph{superpage promotion} which is a system of aggregating smaller pages together as they are used.
Current implementations of hugepages in Linux, in contrast, offer a set of large memory pages available to the kernel for memory allocation.
Unlike superpages, hugepages must be allocated at boot, rather than being coalesced dynamically, but the performance of the Linux hugepages are similar except for increased memory consumption due to unused portions of hugepages.
For virtual machines that are potentially using multiple levels of memory page walk, thereby increasing the latency of those operations, any reduction in the frequency of lookups is beneficial.
Hugepages are not standard practice, however, so hugepages are included as an important mechanism for improving memory performance.

\subsection{I/O Virtualization}
\label{sec:vt_io}
After achieving performant virtualization solutions for compute and memory, the last component in achieving full system virtualization is the virtualization of devices and I/O (IOV).
Along with compute and storage, processing of I/O is one of the most critical components of a server's workload due to the fact that I/O can be a large source of latency for remote transactions.
Efficiently utilizing I/O allows virtual machines to perform nearly any task possible in a native system, particularly the processing of network packets and workloads.
Similar to the ``virtualization penalty'' that occurs with nested page table lookups and binary translation, latency inherent in the processing of network packets with multiple levels of handoffs should be avoided when possible.
Virtualization is essentially software emulation of hardware devices so the natural first attempt at device virtualization is to emulate a device in the kernel, providing a software device to the guest OS that is based on a common physical device.
This method is common in some workstation virtualization products such as VMware workstation \autocite{_jones_1}.
Instead of emulating a software device in the kernel, it is also possible to emulate the device in user-space.
This is the method used by QEMU which provides both device emulation and a platform-agnostic hypervisor.
In Linux operating systems, QEMU is often combined with the KVM modules. 
In this configuration, QEMU provides user-space device emulation for simple devices such as mouse and keyboard, while KVM provides virtualization of the physical hardware.
Userspace emulation has the advantage of removing the responsibility from the kernel, thereby minimizing the potential attack surface of the kernel.
As mentioned earlier, paravirtualized devices are another variation on this theme of emulation where the paravirtualized guest driver communicates with the host paravirtualized devices.
In addition to Xen's paravirtualized driver model, the KVM \texttt{virtio} library is the basis for paravirtualized devices in Linux \autocite{_virtio_1}.
The \texttt{virtio} library takes inspiration from paravirtualization and userspace emulation and uses QEMU to implement the device emulation in userspace so host drivers are not necessary.

While device emulation can provide important flexibility and hardware independence, it brings up the recurring theme of software managing hardware functions which is often less efficient than utilizing dedicated, specialized hardware to perform the function.
The alternative is to avoid any device emulation and allow the guest OS to access hardware directly as if it belonged to the guest rather than the host system.
Since it is conceptually similar to virtualization of the MMU for memory, virtualizing the DMA transactions of modern I/O devices requires an I/O MMU (IOMMU) to allow communication between the guest and I/O devices.
In the x86 architecture, this feature is known as AMD-Vi or Intel VT-d, but both utilize the same concept of an IOMMU to avoid vm-exits when processing I/O.
This allows the hypervisor to unbind a hardware device from its kernel and ``pass through'' or ``direct assign'' the device to the guest OS \autocite{_jones_1}.
Direct assignment of hardware to guests also comes with the high cost of dedicating network interfaces or other important devices to a guest, but these devices provide significant performance improvements over paravirtualized or emulated guest devices, thereby enabling applications that could not previously be virtualized due to low-latency constraints.

If a host system has a large number of virtual machines that need high-performing I/O, it can be difficult to fit enough peripheral cards into the host chassis as passthrough devices to the guest VMs, thus providing each VM with dedicated hardware.
A solution to this apparent conflict of interests can be found in PCI-SIG Single Root I/O Virtualization (SR-IOV) \autocite{_pcisig_1, intelvtd}.
SR-IOV is a general method by which the host system can configure a single hardware device to create and control multiple additional \emph{virtual functions} of the device.
These virtual functions can be directly assigned to and accessed by the guest, similar to passthrough, without removing the main physical function from the host.
The host's physical function represents the original hardware and is responsible for the management functions of the peripheral.  
An illustrative example is the Intel 82599 10 Gigabit network card, used later in this study.
Each physical network interface (physical function or pf) in these network cards has 64 Rx/Tx queue pairs that are normally used as send and receive buffers to maintain multiple simultaneous flows.
The individual queue pairs may be ``broken out'' of the main pool to create a virtual function that is, essentially, a new network device sharing the same physical interface as the physical function.
These new devices are assigned unique MAC addresses and IP addresses to differentiate them on the network so that an outside observer cannot tell them apart from a physical device.
The advantage with SR-IOV is that one interface can be multiplexed into multiple independent interfaces that can each reach near-native performance levels \autocite{_nasa_1}.

\section{Container Architecture}
\label{sec:container_arch}
At a high level, containers can be viewed as another level of access control beyond the traditional user and group permission systems. 
While those systems provide resource access control, containers can provide these resources with finer granularity, thus allowing only those resources and privileges that are required by the process \autocite{_felter_1}. 
In a sense, containers are applying bounds to program execution that should logically have been included in process control from the very start so it is encouraging to see increased interest in their performance and security.

\subsection{Operating System Virtualization}
\label{sec:os_vt}
Also known as \emph{operating system virtualization}, containers are a construct of the operating system that serves to provide limited context, and resources to a process executing on the host system. 
The subsystems that enable a containerized process, including \texttt{chroot}, \texttt{cgroups}, and \texttt{namespaces}, have been gradually added to the Linux kernel over time.
Much like the limitations imposed on applications running sandboxed in mobile devices, all processes on a server or desktop system should have a view of the system that is limited in context and scope -- they should only have access to the resources that they require with some margin for error.  
One of the most important duties of the kernel is to control access to the system's resources including CPU, memory, and I/O.  
In a sense, containers represent a point in the evolution of the kernel where it is truly isolating processes from the system. 

An early implementation of process isolation that led to containers was the restriction of an executing process' file system scope, embodied in the \texttt{chroot} system call and program. 
The \texttt{chroot} call has its origins in BSD Jails and Solaris Zones \autocite{_zones_1}.
Next on the container scene was the addition of \texttt{cgroups} to the Linux kernel.  Contributed by Google \autocite{googlecgroups}, \texttt{cgroups} serve to limit the resources available to a process in a hierarchy, enabling child processes to inherit their parent's \texttt{cgroup} assignment. 
\texttt{Namespaces}, also a recent addition to the kernel, are the mechanism by which the scope or resource view of the process may be limited and controlled.
Linux capabilities are the mechanism by which the monolithic permissions granted to the root user for full system administration are broken down into more granular permissions.  
All of these subsystems, however, are not involved during process execution, but during scheduling and resource assignment by the kernel.  

\subsection{Container Systems}
\label{sec:container_systems}
While Docker has recently popularized process containers, related concepts have been used in earlier operating systems.
FreeBSD introduced the Jails concept in 1999 as a process isolation technique \autocite{_zones_1}.  
The Solaris-variant of the Unix operating system added the concept of Zones (Solaris Containers) in 2004, an upgrade of the BSD Jails concept.  
Other containers have come before (Linux VServer, OpenVZ, FreeBSD Jails, Solaris Zones) and even LXC more recently.  
LXC is the current default Linux container standard, added to Linux as an attempt to bring containers into the mainline kernel \autocite{dockersecurity1}.
The Linux container infrastructure has evolved some as have the many other container systems to the point that there was need to unify the standards on a single container format, \texttt{libcontainer}, which has recently been adopted by the Open Container Initiative under the Linux Foundation \autocite{_oci_1}.  

The Open Container Initiative (OCI) was formed as a collaborative project under the Linux Foundation \autocite{_oci_1}.
Members of this organization span the industry from major cloud players like Microsoft and Amazon to virtualization vendors such as VMware and CoreOS.  
The goal of this new organization is to promote an open, standardized format for containers and the engine runtime libraries. 
They even have a head start on the process with Docker donating its container format, \texttt{libcontainer}, and runtime, \texttt{runC}, to the organization as a starting point.
The development of a common container and runtime for Linux under the OCI should accelerate the standardization and adoption of containers under Linux.

Although Docker may not be representative of container systems as a whole, the majority of the subsystems used by Docker to instantiate containers are part of the Linux kernel so it is expected that containers utilizing these technologies should perform similarly.  
The Linux subsystems supporting containers have become sophisticated enough that Docker or LXC are no longer strictly necessary to create containers, because administrators can create their containers with native Linux  tools such as \texttt{systemd} or just scripts that stitch together the correct \texttt{cgroups}, \texttt{chroot}, and \texttt{namespace} allocation. 
The \emph{de facto} standard for containers in Linux, however, is Docker, based on the amount of media coverage and hype surrounding the young startup.  
Docker is popular for good reason: in addition to providing a robust tool set for isolating processes, the Docker team has been extremely open about their development and responsive to issues submitted by users on their \texttt{github} account \autocite{githubdocker}.  
It is for the reasons of its ubiquity and ease of use that Docker is the container format selected for comparison in this thesis.


The architecture of Docker is based on the underlying Linux systems combined with user space tools written in Go \autocite{dockerarch1}. 
\begin{figure}
    \centering
    \includegraphics[width=150mm]{dockerecosystem.png}
    \caption{Docker ecosystem architecture}
    \label{fig:dockerecosystem}
\end{figure}
The ecosystem of Docker is what makes it so powerful.  
In Figure~\ref{fig:dockerecosystem}, borrowed from \autocite{dockerarch1}, the overall flow of a container is shown as it moves from registry to host and then controlled through the \texttt{docker} client.  


\subsection{Resource Scope}
\label{sec:resource_scope}

Introduced in Version 7 Unix in 1979, one of the earliest Linux systems to enable some kind of operating system virtualization is the \texttt{chroot} system call.
It was later expanded to become FreeBSD Jails and, later, Solaris Zones \autocite{_zones_1}.  
This tool allows a user to change the root directory of a process and its children to a specified directory such that the process subsequently views that directory as its root directory, \texttt{/}.  
This is useful for providing partial file system isolation to processes thus controlling libraries and binaries that are available to the process.
Although this works well for a normal process, it does not prevent a malicious process from accessing arbitrary \texttt{inodes}, so it can be easily circumvented.
It makes this process more difficult, thereby ``hardening'' the target, but was not intended to fully sandbox a process or restrict its file system system calls.
The Linux capability required for \texttt{chroot} is \texttt{cap\_sys\_root}. 

Some of the techniques common in virtualization, such as protecting certain instructions, have been extended into containers as the protection of system capabilities such as \texttt{cap\_sys\_nice} and \texttt{cap\_sys\_chroot}.  
Essentially, if a container needs this capability, it must be explicitly assigned and cannot be ``trapped'' in the host kernel as is common in virtualization.

Linux namespaces: Namespaces and \texttt{cgroups} have been included only for a short time since version 3.8 of the Linux kernel.  

TODO expand on capabilities


\subsection{Resource Control}
\label{sec:resource_control}
Resource Control (cpu/mem scope too): 
Linux \texttt{cgroups} \autocite{kernelcgroups}.

"Control Groups provide a mechanism for aggregating/partitioning sets of tasks, and all their future children, into hierarchical groups with specialized behaviour."

Cgroups are organized into hierarchies with the root being the default \texttt{cgroup} for all processes.  Child processes can be organized into sub-trees that subdivide the resources of its parent process.  Each process must belong to one and only one \texttt{cgroup}. 

Linux capabilities

Subdivision of the capabilities of the root user

Implications \& differences between VMs and containers

since containers need not emulate a second kernel or set of virtual hardware

\subsubsection{Docker networking} % (fold)
\label{ssub:dockernetworking}

Docker networking can be fairly complex with multiple options available to both the daemon and the run commands \autocite{dockernetworking1}.

There are a few standard varieties of network interfaces that are commonly used in virtual environments.
These include both hardware and software options with varying degrees of versatility and performance.  
Most common among software options, virtual bridges are operated by the host kernel with a non-trivial CPU involvement. 
Software bridges are also commonly used by virtual machines and often combined with the \texttt{virtio} library to provide a paravirtualized interface to the guest \autocite{_virtio_1}. 
Bridges are the \emph{de facto} layer 2 networking used by Docker \autocite{dockernetworking1}.  
The Docker daemon also uses the common Linux firewall IPTables for layer 3 connectivity, routing, and network address translation (NAT).
Docker also uses \texttt{iptables} and its NAT functionality to allow the container to communicate with the outside world.
Another reason to study the bridged networking paradigm is that bridges are important in networking in general and useful in systems where containers may be linked together or their functions pipelined.
Additionally, the popular software switch package, Open vSwitch (OVS) may be configured to use virtual bridging as its default network topology.

% subsubsection dockernetworking (end)

\section{Comparisons and Performance Analysis} % (fold)
\label{sec:comparisons_performance_analysis}
Some of the important details of the architecture of virtual machines and containers was described in Section~\ref{sec:vmarchitecture}.
An interesting summary of the differences between virtual machines and containers comes from \autocite{morabitohypervisors}: ``Hypervisors abstract hardware, containers abstract the operating system''. 
This concisely summarizes the important difference between these two types of virtualization in that virtual machines must emulate hardware devices with software which almost always has some performance impact.  
Virtualization of the operating system, however, need not create additional overhead with containers, but use alternate paths through schedulers, etc. to operate the container. 

\begin{figure}
    \centering
    \includegraphics[width=150mm]{vt_schematic.jpg}
    \caption{Architecture overview of virtual machines and container}
    \label{fig:vtschematic}
\end{figure}
Many studies to date have compared the performance of virtual machines and containers.
Figure~\ref{fig:vtschematic} provides a simplified overview of their architecture. 
Virtual machines have multiple additional libraries and operating system instances, which increases the latency of protected operations.  

Felter et al. made an extensive evaluation of containers and virtual machines including memory, compute, and I/O, but they did not attempt to tune the systems \autocite{_felter_1}.
They also used the advanced multi layered unification filesystem (AUFS) which is deprecated in upstream kernels.

Scheepers discusses Xen and LXC macro benchmark comparison \autocite{_scheepers_1}.  
He showed that public clouds need virtual machines for their isolation potential, but containers can be used to maximize the \emph{utilization} of the hardware.

Wang et al. studied the performance overhead of dynamic resource allocation and found that static allocation performed better in most cases \autocite{wangAllocation2007}.
The performance degradation of Xen virtual machines was attributed to intensive network I/O.
This degradation could be addressed in part with static allocation of network interfaces and physical device assignment.
They concluded that decreasing the switching magnitude and frequency of their allocation schedule led to performance improvements.
At the lower limit of switching frequency is static allocation of resources, with processes pinned to CPUs and dedicated physical resources.

Sherry et al. examined Middleboxes which are the heart of NFV \autocite{sherry2012making}.
They show that, as demand for streaming data increases, the need for additional middleboxes to process and handle the traffic is also increasing.
These systems are increasingly assigned to virtual machines due to the flexibility and scaling that can be achieved with increased density.

Middleboxes are also examined in a paper by Sekar et al., where they make the case that middlebox innovation, i.e. NFV, is at least as important as for switches and routers \autocite{sekar2011middlebox}.
They propose that the role of middleboxes in the network comprise the critical usability and security considerations of network design.
One idea presented in their article that has significant potential with containers is the chaining of middlebox functions and its analogy to processing operations on a packet.  
This is one of the core ideas of NFV in that the entirety of the network functions need not be accomplished in one system or even one physical box, but can be distributed around the network as processing nodes.  
By their estimates, there also appears to be a significant amount of common traffic overlap ranging from 64 to 99\% in middlebox processing. 
While this seems shockingly high, it also presents an interesting alternative use case for containers and virtual machines which may be pipelined and even potentially share resources to decrease the redundancy present in current schemes.

Xavier et al. examine the suitability of Linux vServer, OpenVZ, and LXC for high performance computing environments \autocite{xavier2013performance}.
While the high compute-density of containers is an advantage in HPC, it is often the network that has the greatest impact on system performance.
Containers lack the necessary isolation for co-located workloads, but, since HPC workloads and cluster topologies are often controllable and predictable, the lack of isolation in containers should not pose a significant impact to performance.
One of the shortcomings of their study, however, was not using high-throughput network interfaces that are so critical in HPC, which makes it difficult to properly draw distinctions among the systems.
That comparison, specifically since it was targeted at HPC, would have benefited from more rigorous CPU isolation and tuning.  

The work of Wang and Ng analyzed the impact of virtualization on network performance in the Amazon cloud \autocite{wangAmazon2010}.
Although that work found that Amazon does not have the same emphasis on performance as an HPC cluster, there are instances available in the Amazon cloud that completely isolate their CPUs for guaranteed performance.  
Although Wang and Ng did not have access into Amazon's orchestration or control systems, the high-performing virtual machines were not likely oversubscribed with other systems sharing their CPUs. 
These high-performing virtual systems are the subject of this thesis and it is the goal of the performance tuned kernel described in Section~\ref{sub:software_environment}.  

Rathore et al. compared KVM and LXC performance and isolation in hardware-assisted virtual routers, but used dynamic allocation which can kill latency \autocite{rathore2013kvm}.
Their isolation could have improved with realtime tuning and CPU isolation.  
Containers alone cannot provide real isolation of processes without explicit CPU control with \texttt{cgroups}, but the organization of processes and CPU assignment can be tricky.
Orchestration of containers and virtual machines needs to take realtime tuning into account for maximal I/O performance.

There is a significant interest in utilizing virtual systems for HPC.  Another study that investigated this paradigm was performed by Younge et al., where they performed an in-depth analysis of current virtualization technologies in HPC \autocite{_younge_1}.
They investigated a set of current hypervisors for performance and help to guide the choice of hypervisors in this thesis. 
In their article, they examine VMware, KVM, Xen, and VirtualBox hypervisors. 
VMware has previously required permission prior to publishing which complicates using those systems in academic research.  
The proprietary nature of their software also limits its utility in research that seeks to better understand their operating system.  
The popular Xen hypervisor, while open source, requires paravirtualization which creates additional burden.
VirtualBox, while widely available, is also not open source and has little available performance data.  
KVM, the open source hypervisor is selected due to its ubiquity and deep library of literature supporting it.
This choice is also supported by \autocite{_younge_1} who also proclaim it the best performer in internode bandwidth.  

One of the considerations when designing the performance analysis of network systems is the composition of the traffic that they handle.  
Virtual systems often reside in data centers where it has been reported that TCP traffic represents a share of about 90\% if the overall traffic \autocite{haTCPCloud2013}.
This observation highlights the need for performant network interfaces due to the high overhead required in maintaining TCP flows.

While many of the performance analyses described here are focused on high I/O performance for virtual systems, they all declined to investigate the impact of kernel configuration or static resource allocation to improve the network performance of those systems.  
In Section~\ref{sec:experimental_design} to follow, 


% section comparisons_performance_analysis (end)

\nocite{_dpdk_1, _adams_1, _chowdhury_1, _grinberg_1, seo2014performance, gomes2014performance, kivity2014osv, wang2011understanding}


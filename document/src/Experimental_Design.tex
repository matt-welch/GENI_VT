\lstset{basicstyle=\ttfamily\color{black}}
\chapter{Experimental Design}
\label{cha:experimental_design}
\label{sec:experimental_design}
As described in Section~\ref{sec:comparisons_performance_analysis}, the performance comparisons that have been made between containers and virtual machines have, for the most part, not tuned their systems for the comparison.
Tuning can be a daunting task, indeed, due to the numerous potential parameters and the resulting massive number of configuration combinations possible.  
The multitude of parameters causes the number of comparisons to be made among the effects of various tuning parameters to grow exponentially with each additional parameter.
To avoid this potential explosion in the scope of the comparison, performance was only compared herein between only two system configurations and two network types for each type of virtual environment.
The two configurations include the standard Linux kernel, version 3.18.20, and the same kernel with preempt-rt patches applied \autocite{preemptrtpatches} and some important tuning parameters for the kernel and environment.
Network interfaces compared include the standard bridged networking for each virtual system and a physical interface in passthrough to a VM or assigned to the namespace of a container.

\section{Experimental Setup} % (fold)
\label{sec:experimental_setup}
In this section, the experimental environment constructed for this study will be described comprising system hardware, operating system versions, docker parameters, QEMU parameters, kernel versions, kernel parameters, and kernel configurations.

\subsection{Hardware} % (fold)
\label{sec:hardware}
The motherboard used for the test system was a SuperMicro X10DRH-C/i server board with dual CPU sockets.
Both CPU sockets on the motherboard were populated with an Intel Xeon E5-2608L v3, having 6-core running at 2.00 GHz.  
Hyperthreading, CPU Turbo, P-states, and C-states were all disabled in the BIOS as a performance optimization.  
The platform was populated with with two 16GB memory sticks per socket for a total of 64 GB of DDR4 memory running at 1866 MHz in Dual-channel mode.  
The network card used was an Intel 82599ES 10-Gigabit PCIe card with two optical interfaces, each directly connected to the client system.  
The 10 Gigabit network interfaces were used in order to stress the performance of the virtual systems.  
The computation involved in processing a 1Gbps flow is generally not significant enough to stress a virtual machine, whereas a 10 Gbps stream provides a significant workload in order to observe significant differences between the systems.

% subsection hardware (end)

\subsection{Networking and Topology} % (fold)
\label{sub:expt_networking}
\subsubsection{Software Networking} % (fold)
\label{ssub:software_networking}
As was discussed in Section~\ref{sec:dockernetworking}, the default networking paradigm of Docker is to create a bridge and pair of virtual Ethernet interfaces \autocite{dockernetworking1}.
Both interfaces are bound to the bridge, one remains in the host’s namespace and the other is assigned to the container namespace as its primary network interface.
Figure~\ref{fig:topology_bridged_vm} shows the topology of a bridged network connection to a virtual machine or container as was configured herein.  
IPTables will also have an impact on the bridge performance, but it was kept part of the configuration since it represents the \emph{de facto} networking configuration of Docker.
No effort was made to optimize the performance of the bridge.
Parameters including whether the bridge participates in the STP algorithm are optional and can affect performance.
The focus of this work is not on tuning the network interfaces themselves, however, but rather investigates how a latency-sensitive system can be configured to improve performance.
\begin{figure}
    \centering
    \def\svgwidth{\columnwidth}
    \input{topology_bridged_VM.pdf_tex}
    \caption{Topology of bridged network}
    \label{fig:topology_bridged_vm}
\end{figure}

% subsubsection software_networking (end)

\subsubsection{Hardware networking} % (fold)
\label{ssub:hardware_networking}
In order to obtain optimal performance for both virtual machines and containers, a physical interface may be passed from the host into the domain of the virtual machine or into the namespace of the container.
As was described in Section~\ref{sec:vt_io}, this is known as passthrough for virtual machines and is supported by CPU virtualization extensions \autocite{_grinberg_1}..
In the version of Docker used herein, 1.8.1, direct physical assignment is not enabled by default.
Instead, a fairly simple script was written, with inspiration borrowed from J\'{e}r\^{o}me Petazzoni's pipework \autocite{pipework1jpetazzo}, Jason Anderson's direct-phys fork of pipework \autocite{pipework1Rakurai}, and a serverfault.com post \autocite{serverfault1} to assign a physical interface to the container namespace. 

% subsubsection hardware_networking (end)

% subsection expt_networking (end)

\subsection{Software Environment} % (fold)
\label{sub:software_environment}
The operating system used in this study needed to have a balance of stability, advanced features, and performance, so a Long Term Support version of Ubuntu, 14.04.3 LTS was chosen.
The Linux 3.18.20 kernel was selected for both the host and guest OS due to a set of factors affecting usability and performance.
Choosing which Linux kernel version to use for this study was complicated by factors that were not known until system configuration.
It was already understood prior to configuration that Docker was not supported in kernel versions prior to 3.10 which helped to narrow down the possibilities.
One of the side-goals of this study was to use as much standard and open software as possible so that the kernel and system configurations could be easily reproduced by other investigators.
This goal had a substantial influence on the choice of kernel due to the performance impact of the Docker's storage driver.
By default, Docker currently  uses the AUFS filesystem for its back-end storage driver.
This module allows the classical union filesystem approach of masking important host files and providing private copies of system files to each container.
Other options for this driver include devicemapper, zfs, btrfs, and overlayfs.
The AUFS driver has been deprecated in the Linux kernel so it is not available upstream which disqualifies it from any forward-looking uses.
The devicemapper driver, while ubiquitous, shows significant performance degradation and usability issues disqualifying it as well.
Both zfs and btrfs have stability issues and are not yet standard filesystem drivers so they were also disqualified.
The remaining choice was overlayfs which was recently upstreamed into the 3.18 kernel and seems to be the choice for Docker storage driver going forward until zfs and btrfs become standard \autocite{petazzonistorage, redhatstorage}.
Further limitations, while not as impactful as the choice of the 3.18 kernel include the availability of preempt-rt patches for the kernel.

\subsubsection{Kernel Configuration} % (fold)
\label{ssub:kernel_configuration}
One of the steps involved in tuning the RT variant of the kernel was to apply a specific set of kernel configuration choices during the kernel build process.
The kernel shipped with Ubuntu 14.04.3 was version 3.19.0-25-generic.
That kernel's default configuration (.config file) was used as the basis for the configuration and building of the test kernel version 3.18.20. 
This default kernel configuration omits even simple optimizations such as disabling CPU frequency scaling, and enabling a preemptable kernel, but the default kernel configuration is probably the most common on servers that have not done significant performance tuning and optimizations so it seemed appropriate. 
The "performance" kernel used was the 3.18.20-rt18 kernel which is the 3.18.20 kernel with the 3.18.20-rt18 patches, posted 2015-08-11, from \autocite{preemptrtsource} applied.
The initial kernel configuration used was that of the 3.18.20 kernel. 
Only a few important parameters of the kernel were modified to tune the systems for performance with the preempt-rt patch.
Kernel CONFIG modifications are described in Table~\ref{tab:kernel_config_table}.  

\input{kernel_configuration_table.tex}
% subsubsection kernel_configuration (end)

\subsubsection{Kernel Boot Parameters} % (fold)
\label{ssub:kernel_params}
Another important mechanism for tuning a kernel is to modify the parameters passed to the kernel at boot time \autocite{linuxkernelparams}.  
Standard kernels usually boot with only a few parameters passed to the kernel such as "ro quiet" which will make the kernel read-only and suppress most kernel boot messages.  
The preempt-rt kernel used herein, Linux 3.18.20-rt18, was booted with a specific set of additional parameters to improve its performance and determinism for guest virtual machines and containers.
The set of parameters passed to the "performance" kernel are summarized in Table~\ref{tab:kernel_params_table}
\input{kernel_parameters_table.tex}
One of the most important tuning parameters in a virtual machine host relates to the level of over-subscription for each physical CPU.
Systems with low performance requirements may stack multiple guest vCPUs (virtual CPUs) on each physical CPU.  This stacking can lead to multiple vCPUs from unrelated virtual machines sharing the pCPU or just host processes running on the pCPU assigned to the guest.
Either of these scenarios can have a large impact on guest latency.
If a pCPU is running another task when a guest vCPU is not in the current context, the additional latency required to change context to the guest and resume the guest OS operation can be a significant source of latency.
These resource conflicts can be all but eliminated by "unplugging" pCPUs assigned to a guest from the host scheduler with the isolcpus=X kernel boot parameters.
Assigning a pCPU to the isolcpus list, like isolcpus=2, will remove that pCPU from the kernel scheduler's CPU pool, preventing the kernel scheduler from running any processes there.
The isolated CPUs can still be used to run user processes with Linux utilities like taskset or numactl, but will remain quiescent until that happens.
Scalability of a system with isolated CPUs is reduced, but in an era of server processors with more and more cores, the impact of dedicating individual pCPUs to latency-sensitive processes is reduced.

Access to memory can be one of the greatest sources of latency in computationally intensive processes.
Virtual machines have a similar sensitivity to latency in that memory accesses can require both guest and host-level page walks.
In order to minimize the consequences of the multi-level memory lookups, hugepage memory was used as the basis for the RT guests memory.  
As was discussed in Section~\ref{sec:vt_memory}, hugepages can greatly reduce the cost of memory lookups and will significantly impact the performance of virtual machines.
The host system enabled hugepages in the kernel with the kernel arguments \lstinline{hugepagesz=2M hugepages=4096}, which allocated 4096 hugepages of 2 MB each.  
\\

% subsubsection kernel_params (end)

\subsubsection{Building a Virtual Machine} % (fold)
\label{ssub:build_vm}
The image used for virtual machine testing was based on a raw 16 GB disk image created with the command line tool \lstinline{qemu-img}.
Ubuntu server 14.04.3 was then installed to the image from the ISO file and only the OpenSSH package was selected during the installation.  
After Ubuntu was installed, the packages for vim, ethtool, screen, qemu-kvm, exuberant-ctags, apparmor, bridge-utils, and libpcap-dev were installed in the VM to keep it consistent with the host installation.  


% subsubsection build_vm (end)

\subsubsection{QEMU and hypervisor parameters} % (fold)
\label{ssub:qemu_params}
QEMU is responsible for device emulation so this is where it is important to set up the virtual machine to be as performant as possible.
For optimal performance inside the performance-tuned virtual machine, QEMU was used to launch the guest virtual machine with some additional command-line arguments to improve guest and hypervisor performance.  
The complete set of additional QEMU parameters is shown in Table~\ref{tab:qemu_params_table}.  
In this work, higher-level libraries such as libvirt or VMware orchestration are not used in favor of running as efficiently as possibly without additional software layers of abstraction impacting performance.
The hope is for maximal performance so minimal abstraction layers were utilized in the system configuration.

\input{qemu_parameters_table.tex}

% subsubsection qemu_params (end)

\subsubsection{Docker parameters} % (fold)
\label{ssub:docker_params}
The Docker environment is logically separated into building with \lstinline{docker build}, back-end management performed by the docker daemon, and the \lstinline{docker run} command that actually instantiates containers from images and assigns the appropriate cgroups and namespacing to the containers.
The docker daemon essentially sets up the software environment for the containers, specifically their filesystems, libraries, and available binaries.  
Along with the files available to the container, the daemon also manages cgroups and namespaces to properly control the container's resources and limit its scope.  
The docker run command is responsible for requesting resources and privileges for the container as it is instantiated.

The image building process of Docker is controlled by the \lstinline{docker build} command line tool.  
This tool takes recipe files known as Dockerfiles \autocite{_dockerfile_1} and constructs the image based on instructions therein.
A Dockerfile usually specifies the base image that should be used for the build along with setting environment variables and installing necessary packages and binaries to the container's filesystem. 
It is consumed by the \lstinline{docker build} tool and the image is constructed as the composite overlay of each step in the build process \autocite{dockerbuild1}.
All of the containers used herein were based on the official Ubuntu image, which is based on the latest release of Ubuntu 14.04 LTS.
The Ubuntu image was chosen in order to maintain a consistent environment with the host and virtual machines.
The image created for benchmarking was called netbench and created with the Dockerfile.netbench.  It only pulled the Ubuntu base image, applied some labels, and installed netperf.  

The daemon has a number of parameters that may be modified to tune parameters such as available cgroup CPU sets, bridged networking, and the storage driver used for container filesystems.
In this study, the docker daemon was launched with mostly standard parameters but for the following exceptions.
As discussed in Section~\ref{sub:software_environment}, the overlayfs storage driver was chosen, requiring the docker daemon to be launched with \lstinline{--storage-driver=overlay}.  
Additionally, docker's default bridge, docker0, was configured to draw on a pool of IP addresses from the subnet 192.168.42.240/28 for its bridged virtual Ethernet interfaces.
The additional arguments used with \lstinline{docker run} and their importance are summarized in \ref{tab:dockerrunparams}.


All containers were run with non-persistent filesystems by using the \lstinline{--rm} option in the docker run line.
This had the effect of removing the container's filesystem overlay after it exits, preventing any filesystem modifications between runs.
Indeed, this is one of the advantages of containers in obtaining consistent performance since the container filesystem is reloaded from the base image at every launch.
Containers were also run with the flags \lstinline{-i -t} or \lstinline{-it}, which requested that an interactive tty be allocated to the container while it is running.
This has the effect of a small additional overhead for the terminal process, but this is consistent with the default operation of Docker.  
Containers are all launched with a specified MAC address for their bridge interface to remove the need to refresh ARP tables with new, randomized MAC addresses.
% subsubsection docker_params (end)

% subsection software_environment (end)

% section experimental_setup (end)

\section{Experimental Procedure} % (fold)
\label{sec:experimental_procedure}
Given the experimental setup described in Section~\ref{sec:experimental_setup}, the performance of virtual machines and containers could be compared.  
\begin{table}[ht!]
    \centering
    \label{tab:environmentvariables}
    \caption{System test configurations}
    \begin{tabular}{|l|c|c|}
    \hline
    Parameter & variants & \\
    \hline \hline
    kernel & 3.18.20 & 3.18.20-rt18 \\ 
    environment & docker & KVM \\ 
    network interface & bridge & physical \\ 
    \hline
    \end{tabular}
\end{table}

We wanted to compare the performance of the systems with tuning to those without and compare the performance of two different network interfaces.
The set of 3 variables and 2 parameters each produced 8 configurations to test.  The variables are summarized in Table~\ref{tab:environmentvariables} 

The performance of latency-sensitive network workloads are often not bound by memory or CPU, but by their ability to process I/O traffic.
Testing herein concentrated on network-related benchmarks to evaluate bandwidth and latency of the various operating system and network configurations.
The network paths that were tested comprised both TCP and UDP flows in addition to ICMP latency.

\begin{table}
    \centering
    \caption{netperf 2.7.0 benchmark subtests}
    \label{tab:netperfsubtests}
    \begin{tabular}{|l|c|c|}
        Subtest & Description \\ 
    \hline
        TCP-STREAM & TCP bandwidth over a 10 second connection \\ \hline
        TCP-RR & TCP Request-Response latency \\ \hline
        UDP-STREAM & UDP bandwidth over a 10 second connection \\ \hline
        UDP-RR & UDP Request-Response latency \\ \hline
    \end{tabular}
\end{table}
Network performance was measured with netperf \autocite{netperfHome} and ping. 
The system under test ran the server process for netperf inside the virtual environment in order to determine how these systems perform in a server context.
Using netperf version 2.7.0 \autocite{netperfManual}, network bandwidth and latency were measured with tests comprising the default bandwidth test TCP\_STREAM, TCP\_RR, UDP\_STREAM, and UDP\_RR.
The subtests are summarized in Table~\ref{tab:netperfsubtests}\\  
For each of the netperf tests, 20 samples were collected.  For the ping test, 100 samples were collected with a 1 second interval.  

% section experimental_procedure (end)

\nocite{rathore2013kvm, iperf3}
